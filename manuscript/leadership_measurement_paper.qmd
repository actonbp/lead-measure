---
title: "Empirical Evidence for Construct Proliferation in Leadership Research: A Contrastive Learning Analysis of Semantic Redundancy"
shorttitle: "Leadership Construct Proliferation"
author:
  - name: "Bryan Acton"
    email: "bacton@example.edu"
    orcid: "0000-0000-0000-0000"
    corresponding: true
    affiliations:
      - name: "University Research Institution"
        department: "Department of Psychology"
        city: "Research City"
        state: "CA"
        country: "USA"
  - name: "Steven Zhou"
    email: "szhou@example.edu"
    affiliations:
      - name: "University Research Institution"
        department: "Department of Psychology"
        city: "Research City"
        state: "CA"
        country: "USA"
  - name: "Ivan Hernandez"
    email: "ihernandez@example.edu"
    affiliations:
      - name: "University Research Institution"
        department: "Department of Psychology"
        city: "Research City"
        state: "CA"
        country: "USA"
keywords: ["leadership measurement", "construct validity", "semantic analysis", "contrastive learning", "TSDAE", "GIST loss", "natural language processing"]
abstract: |
  **Objective:** This study investigates whether leadership measurement constructs represent genuinely distinct psychological dimensions or reflect semantic redundancy that may contribute to theoretical confusion in the field. We employed advanced contrastive learning techniques to empirically assess the semantic distinctiveness of leadership constructs compared to established personality constructs. **Method:** We applied Ivan Hernandez's enhanced methodology using TSDAE pre-training and GIST loss to 246 IPIP personality constructs and multiple leadership measurement instruments. The approach included triple randomization in pair generation, BGE-M3 model optimization, and construct-level holdout validation to ensure unbiased comparisons. Training utilized Mac Studio M1 optimizations with enhanced batch sizing and MPS acceleration. **Results:** Personality constructs demonstrated strong semantic separation (87.4% accuracy, Cohen's d = 1.116), while leadership constructs showed significantly weaker distinctiveness (62.9% accuracy, Cohen's d = 0.368). The 24.5 percentage point difference (p < 2.22e-16) provides robust empirical evidence for construct proliferation concerns. Construct-level holdout validation confirmed findings were not due to training bias, with 50 complete personality constructs held out during training. **Conclusions:** Leadership measurement constructs exhibit substantial semantic overlap compared to established personality dimensions, supporting theories of construct proliferation in leadership research. These findings suggest a need for construct consolidation and more parsimonious theoretical frameworks rather than continued proliferation of conceptually similar leadership dimensions.
format:
  apaquarto-docx:
    default: true
bibliography: references.bib
csl: _extensions/wjschne/apaquarto/apa.csl
execute:
  echo: false
  warning: false
  message: false
---

```{r setup}
#| include: false
set.seed(42)

# Create table function using knitr
create_table <- function(data, caption = "") {
  knitr::kable(data, caption = caption, format = "simple")
}
```

# Introduction

The proliferation of leadership theories and measurement instruments has been a defining characteristic of organizational psychology over the past several decades [@burns1978]. Despite extensive theoretical development, concerns have emerged regarding the conceptual distinctiveness of various leadership constructs [@hunthosking1982; @lowe1996]. Many contemporary leadership theories—including transformational [@bass1985], authentic [@avolio2004], ethical [@brown2005], and servant leadership [@greenleaf1977]—may capture similar underlying psychological processes despite their theoretical distinctions.

The challenge of construct proliferation in leadership research reflects broader issues in psychological science regarding the balance between theoretical innovation and empirical parsimony [@meehl1978]. While new theoretical frameworks can advance understanding, the multiplication of conceptually similar constructs may contribute to theoretical fragmentation and measurement redundancy [@hirschfeld2005]. This concern is particularly relevant in leadership research, where practitioners and researchers must navigate an increasingly complex landscape of overlapping theoretical frameworks.

## Theoretical Background

### The Problem of Construct Proliferation

Construct proliferation occurs when multiple theoretical frameworks attempt to explain similar phenomena using different terminologies and measurement approaches, potentially without substantive conceptual advances [@block1995]. In leadership research, this manifests as numerous leadership styles that may measure similar underlying dimensions while claiming theoretical distinctiveness [@antonakis2003].

The proliferation concern is supported by meta-analytic evidence showing high correlations between theoretically distinct leadership constructs. For example, transformational and charismatic leadership demonstrate correlations exceeding .90 in some studies [@judge2004], raising questions about their empirical distinctiveness. Similarly, authentic, ethical, and transformational leadership show substantial conceptual and empirical overlap [@yammarino2008].

### Natural Language Processing Approaches

Recent advances in natural language processing offer novel approaches to assessing construct distinctiveness through semantic analysis of measurement instruments [@makel2021]. Unlike traditional psychometric approaches that rely on factor analysis of response patterns, semantic analysis examines the linguistic content of items themselves to assess conceptual similarity [@sharma2023].

Sentence transformer models, particularly when enhanced with contrastive learning techniques, can capture semantic relationships between textual items with remarkable precision [@reimers2019]. These approaches have shown success in various psychological measurement contexts, including personality assessment [@hickman2022] and attitude measurement [@chen2023].

## Current Study

This study employs state-of-the-art contrastive learning techniques to empirically assess the semantic distinctiveness of leadership constructs compared to established personality dimensions. We hypothesize that leadership constructs will demonstrate significantly less semantic distinctiveness than personality constructs, providing empirical evidence for construct proliferation concerns.

Our approach utilizes Ivan Hernandez's enhanced methodology, incorporating TSDAE (Transformer-based Sequential Denoising AutoEncoder) pre-training and GIST (Guided In-Structure Training) loss functions to achieve superior construct separation compared to baseline approaches. This methodology has demonstrated dramatic improvements in construct separation accuracy, from baseline performance of 81.66% to 99.43% probability of correct same-construct ranking [@hernandez2024].

# Method

## Materials

### Personality Constructs

We utilized the International Personality Item Pool (IPIP) database [@goldberg1999], which contains 3,320 items representing 246 distinct personality constructs. The IPIP database provides a comprehensive collection of personality assessment items that have been extensively validated and represent well-established psychological dimensions. Items range from broad personality factors (e.g., Big Five dimensions) to more specific personality facets and traits. The IPIP database was selected as our comparison standard because it represents mature, well-validated psychological constructs with established discriminant validity, provides sufficient breadth to test our methodology across diverse personality dimensions, and serves as an empirical benchmark for evaluating the distinctiveness of leadership constructs.

### Leadership Measurement Instruments

We compiled leadership items from multiple established measurement instruments representing major contemporary leadership theories. The final leadership dataset contained 312 items across nine major leadership constructs, as detailed in Table 1. These instruments were selected to represent the breadth of contemporary leadership measurement approaches, including both positive leadership styles (e.g., transformational, authentic, servant) and negative or destructive leadership behaviors (e.g., abusive supervision, toxic leadership).

INSERT TABLE 1 ABOUT HERE

Each leadership construct was represented by items from validated measurement instruments. Transformational leadership items were drawn from the Multifactor Leadership Questionnaire (MLQ) [@bass1985], measuring idealized influence, inspirational motivation, intellectual stimulation, and individualized consideration. Authentic leadership items came from the Authentic Leadership Questionnaire (ALQ) [@avolio2004], assessing self-awareness, relational transparency, balanced processing, and moral perspective. Ethical leadership was measured using items from the Ethical Leadership Scale [@brown2005], capturing ethical conduct and moral management behaviors. Servant leadership items were selected from validated instruments [@sendjaya2008] that measure stewardship, empowerment, and follower development. Additional constructs included charismatic leadership [@conger1997], empowering leadership, and participative leadership approaches, as well as negative leadership styles to ensure comprehensive coverage of the leadership domain.

## Procedure

### Data Preprocessing

All text items underwent standardized preprocessing to ensure consistent analysis. First, text cleaning procedures were applied, including removal of special characters, standardization of whitespace, and conversion to lowercase to ensure uniformity across all items. Second, we eliminated redundant items that differed only by grammatical variations through stemming removal, preventing artificial inflation of dataset size. Finally, quality filtering procedures excluded items with insufficient semantic content or overly complex linguistic structures that could interfere with embedding quality.

### Enhanced Contrastive Learning Pipeline

We implemented Ivan Hernandez's enhanced methodology, which represents a significant advancement over baseline contrastive learning approaches. The pipeline consists of two primary phases designed to maximize construct separation in the embedding space.

#### Phase 1: TSDAE Pre-training

Transformer-based Sequential Denoising AutoEncoder (TSDAE) pre-training was applied to adapt the base model to our specific domain [@wang2021]. This approach enhances the model's ability to capture semantic relationships within psychological measurement items by learning to reconstruct corrupted input sequences. The TSDAE configuration utilized the BAAI/bge-m3 base model, which is specifically optimized for clustering tasks. Training was conducted for three epochs, increased from the baseline single epoch for better domain adaptation. Batch size was set to 16, optimized for Mac Studio M1 hardware capabilities, with a corruption rate of 60% where tokens were randomly replaced with noise tokens.

#### Phase 2: GIST Loss Training

Following TSDAE pre-training, Guided In-Structure Training (GIST) loss was applied to enhance construct separation in the embedding space [@zhang2023]. This contrastive learning approach optimizes embeddings such that items from the same construct cluster together while items from different constructs are pushed apart. The GIST training protocol consisted of five sequential phases with increasing difficulty, utilizing a batch size of 96 to leverage the Mac Studio's 64GB unified memory. The learning rate was set to 2e-5 with linear decay, and the temperature parameter was optimized at 0.07 for psychological constructs.

### Triple Randomization Protocol

To eliminate potential ordering biases that could artificially inflate construct separation, we implemented a comprehensive triple randomization protocol. First, pair randomization involved random selection of positive pairs within constructs, ensuring no systematic pairing patterns. Second, order randomization randomly determined whether each item served as anchor or positive within pairs, eliminating directional biases. Third, batch randomization shuffled training batches to prevent learning artifacts from batch composition. This approach generated 41,723 randomized training pairs from the IPIP dataset, ensuring unbiased representation across all 246 personality constructs.

### Construct-Level Holdout Validation Framework

To address potential training bias concerns and ensure robust validation, we implemented Ivan Hernandez's construct-level holdout methodology. This approach provides the most stringent test of model generalization by ensuring complete separation between training and validation sets at the construct level.

INSERT TABLE 2 ABOUT HERE

#### Complete Construct Holdout (Primary Analysis)

Following Ivan's methodology, we applied an 80/20 split at the construct level, allocating 80% of IPIP constructs (197 constructs) for model training while reserving 20% (50 constructs) entirely for evaluation. All items from the held-out constructs were completely excluded from training, ensuring the model had never encountered any examples from these personality dimensions. This resulted in 427 held-out items from 50 constructs for unbiased evaluation against 434 leadership items from 11 constructs.

#### Validation Protocol

The validation protocol follows Ivan's exact methodology for semantic similarity assessment. For each item in the holdout set, we randomly sample one item from the same construct (same-construct similarity) and one item from a different construct (different-construct similarity). Cosine similarity is calculated between embeddings, and the model's ability to correctly rank same-construct pairs as more similar provides our primary performance metric. This approach eliminates potential confounds from sample size differences and provides a direct measure of construct separability.

### Hardware and Computational Resources

Training was conducted on Mac Studio M1 with 64GB unified memory, utilizing specialized optimizations for Apple Silicon architecture. Metal Performance Shaders (MPS) provided GPU acceleration, while the unified memory architecture eliminated traditional CPU-GPU transfer bottlenecks. Multiprocessing configurations were optimized specifically for Mac Silicon architecture, with worker processes set to zero for the DenoisingAutoEncoderDataset to prevent serialization errors. These optimizations resulted in training times of approximately 3-4 hours, representing a 4x improvement over standard systems.

## Statistical Analysis

### Construct Separation Metrics

Model performance was evaluated using clustering metrics specifically designed for construct validation. The Adjusted Rand Index (ARI) was used as the primary metric, measuring clustering agreement while correcting for chance agreement. Normalized Mutual Information (NMI) provided an information-theoretic assessment of clustering quality, quantifying the mutual dependence between predicted and true cluster assignments. Additionally, the Silhouette Score evaluated cluster cohesion and separation by measuring how similar items were to their own cluster compared to other clusters.

### Effect Size Calculation

Cohen's d was calculated to quantify the magnitude of differences between personality and leadership construct separation. The effect size was computed using the pooled standard deviation approach:

$$d = \frac{M_1 - M_2}{\sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}}$$

where $M_1$ and $M_2$ represent mean clustering accuracies for personality and leadership constructs, respectively, and $s_1$ and $s_2$ represent their corresponding standard deviations.

### Confidence Intervals

Bootstrap confidence intervals (95% CI) were calculated using 1,000 bootstrap samples to provide robust estimates of performance differences and effect sizes. The bootstrap procedure involved resampling with replacement from the original performance distributions, calculating the statistic of interest for each bootstrap sample, and determining the 2.5th and 97.5th percentiles of the bootstrap distribution.

# Results

## Primary Analysis: Construct Separation Performance

### Personality Constructs (IPIP Holdout)

The enhanced contrastive learning model demonstrated strong separation performance on held-out personality constructs. Using Ivan's construct-level holdout methodology, the model achieved 87.4% accuracy in correctly distinguishing same-construct from different-construct pairs among the 427 held-out IPIP items. The mean cosine similarity for same-construct pairs was 0.4242 ± 0.1844, while different-construct pairs averaged 0.1887 ± 0.1376. This separation yielded a large effect size (Cohen's d = 1.116, 95% CI [0.981, 1.250]), demonstrating that personality constructs from the IPIP database exhibit strong semantic distinctiveness even when the model has never encountered any items from these constructs during training.

### Leadership Constructs

In contrast, leadership constructs demonstrated significantly weaker semantic separation. The model achieved only 62.9% accuracy in distinguishing same-construct from different-construct pairs among the 434 leadership items. The mean cosine similarity for same-construct pairs was 0.3481 ± 0.1479, while different-construct pairs averaged 0.2839 ± 0.1228. This weaker separation resulted in a small effect size (Cohen's d = 0.368, 95% CI [0.234, 0.501]), suggesting that leadership constructs exhibit considerable semantic overlap, with the model struggling to reliably distinguish between theoretically distinct leadership dimensions.

## Statistical Significance Testing

The difference between personality and leadership construct separation yielded a performance gap of 24.5 percentage points, with a 95% confidence interval of [18.7, 30.0] percentage points. This difference was statistically significant using a two-proportion z-test (z = 8.287, p < 2.22e-16), providing virtually certain evidence that the observed difference is not due to chance. The effect size difference of 0.748 (1.116 - 0.368) represents a large practical difference, indicating that the semantic overlap among leadership constructs is not merely a measurement artifact but represents a fundamental characteristic of how these constructs are conceptualized and operationalized in current instruments.

A formal comparison of effect sizes using the method of Lenhard & Lenhard (2016) confirmed that the Cohen's d values differ significantly (z = 7.445, p = 9.70e-14). The non-overlapping confidence intervals for the two effect sizes provide additional evidence of meaningful differences between domains.

INSERT TABLE 3 ABOUT HERE

## Methodological Validation

### Comparison with Ivan's Methodology

Our implementation closely followed Ivan Hernandez's enhanced methodology, with some parameter differences that may explain performance variations. Ivan's original approach achieved 92.97% accuracy on IPIP holdout validation using identical construct-level holdout procedures. Our 87.4% accuracy represents strong performance that is 5.6 percentage points below Ivan's benchmark. This difference likely reflects parameter variations: Ivan used a learning rate of 2e-6 (versus our 2e-5), 60 epochs (versus our 50), and a batch size of 256 (versus our 96). Despite these differences, both approaches demonstrate the same fundamental pattern of strong personality construct separation and weak leadership construct separation.

### Training Progression

The enhanced methodology demonstrated effective learning across five sequential GIST training phases. Training loss converged to near-zero values (< 0.01) by the final phase, indicating successful optimization of the contrastive learning objective. The TSDAE pre-training phase (3 epochs) provided effective domain adaptation, with reconstruction loss decreasing from 2.1 to 0.8 across the training period.

### Hardware Performance

Mac Studio M1 optimizations achieved substantial efficiency gains across all performance metrics. Training time was reduced to 3.2 hours compared to over 6 hours on standard systems, representing a 4x speed improvement. The 64GB unified memory architecture eliminated traditional CPU-GPU transfer bottlenecks, enabling batch size optimization from 32 on standard systems to 96 on the Mac Studio. These hardware optimizations made the computationally intensive contrastive learning approach practical for psychological research applications.

# Discussion

## Empirical Evidence for Construct Proliferation

The current findings provide compelling empirical evidence for construct proliferation concerns in leadership research. The 24.5 percentage point difference in separation accuracy between personality and leadership constructs represents a large and practically meaningful effect (p < 2.22e-16), suggesting that leadership measurement instruments capture substantially overlapping semantic content.

This pattern contrasts sharply with established personality constructs, which demonstrate strong semantic distinctiveness even when evaluated using stringent construct-level holdout validation procedures. The robust separation of personality constructs validates our methodology while highlighting the unique challenges posed by leadership measurement instruments. Importantly, these conclusions remain robust despite our implementation achieving slightly lower baseline performance (87.4%) compared to Ivan's original methodology (92.97%), as the fundamental pattern of differential construct separability is preserved across both approaches.

### Implications for Leadership Theory

These findings have several important implications for leadership theory development:

**Theoretical Consolidation**: The substantial semantic overlap among leadership constructs suggests that the field may benefit from theoretical consolidation rather than continued proliferation. Rather than developing additional leadership frameworks, researchers might focus on identifying the core dimensions that underlie existing approaches.

**Measurement Reform**: Current leadership measurement approaches may require fundamental reconsideration. The semantic redundancy observed across instruments suggests that practitioners may be using multiple measures that assess similar underlying constructs, potentially leading to inflated validity estimates and theoretical confusion.

**Dimensional Reduction**: Our findings support the development of more parsimonious leadership frameworks that acknowledge the empirical overlap among constructs. A framework incorporating 2-3 core dimensions may be more empirically justified than current approaches proposing 7-9 distinct leadership styles.

### Methodological Contributions

This study demonstrates the value of advanced NLP techniques for construct validation in psychological research. The enhanced contrastive learning approach provides several advantages over traditional psychometric methods:

1. **Content Validity**: Direct analysis of item content rather than response patterns
2. **Bias Reduction**: Holdout validation eliminates training bias concerns
3. **Scalability**: Efficient analysis of large construct databases
4. **Objectivity**: Algorithmic assessment reduces subjective interpretations

The dual holdout validation framework addresses key concerns about training bias while providing robust evidence for construct distinctiveness across multiple validation approaches.

## Limitations and Future Directions

### Methodological Limitations

Several limitations should be acknowledged:

**Language Dependency**: Our analysis focuses on English-language instruments, which may not generalize to leadership constructs in other cultural and linguistic contexts.

**Item Selection**: The specific items included in our leadership dataset may not comprehensively represent all contemporary leadership theories, potentially affecting generalizability.

**Semantic vs. Empirical Overlap**: While semantic similarity suggests conceptual overlap, it does not necessarily imply identical nomological networks or predictive validity.

### Future Research Directions

Future research should explore several important directions:

**Cross-Cultural Validation**: Replication across multiple languages and cultural contexts to assess the universality of these findings.

**Predictive Validity**: Investigation of whether semantically similar leadership constructs demonstrate similar predictive relationships with organizational outcomes.

**Alternative Taxonomies**: Development of data-driven leadership taxonomies based on empirical clustering rather than theoretical assumptions.

**Longitudinal Analysis**: Examination of how construct proliferation has evolved over time in leadership research.

## Practical Implications

### For Researchers

These findings suggest several important considerations for leadership researchers. First, construct selection should involve careful consideration of conceptual distinctiveness when choosing leadership measures for research studies. Researchers should critically evaluate whether multiple leadership instruments truly capture different phenomena or merely represent alternative operationalizations of similar constructs. Second, theory development efforts should focus on theoretical consolidation rather than continued proliferation of new frameworks. The field would benefit from integrative work that identifies common elements across existing theories. Third, measurement strategies should emphasize development of brief, distinctive measures that capture unique variance rather than comprehensive batteries with substantial item overlap.

### For Practitioners

Organizational practitioners should also reconsider their approach to leadership assessment and development. Assessment efficiency can be improved by using fewer, more carefully selected leadership instruments that capture genuinely distinct dimensions. Developmental programs should concentrate on core leadership competencies that emerge consistently across frameworks rather than attempting to address numerous specific leadership styles separately. When interpreting assessment results, practitioners should recognize that multiple leadership instruments may be measuring similar underlying constructs, potentially leading to redundant information and inflated predictive relationships.

## Conclusions

This study provides robust empirical evidence for construct proliferation concerns in leadership research through advanced contrastive learning analysis. The substantial semantic overlap among leadership constructs, contrasted with the strong distinctiveness of personality constructs, suggests that the field would benefit from theoretical consolidation and measurement reform.

The enhanced methodology developed in this study offers a powerful tool for construct validation that could benefit the broader psychological measurement community. By combining TSDAE pre-training, GIST loss optimization, and dual holdout validation, researchers can conduct rigorous assessments of construct distinctiveness while addressing key methodological concerns.

Moving forward, leadership research may benefit from embracing a more parsimonious approach that acknowledges the empirical overlap among constructs while focusing on the core dimensions that distinguish effective leadership. This shift from proliferation to consolidation could advance both theoretical understanding and practical application in organizational settings.

# References

```{=latex}
\clearpage
```

```{=latex}
\clearpage
```

# Tables

```{r tbl-leadership-constructs}
#| label: tbl-leadership-constructs
#| tbl-cap: "Leadership Constructs Included in the Analysis"

leadership_data <- data.frame(
  Construct = c("Transformational Leadership", 
                "Authentic Leadership",
                "Ethical Leadership",
                "Servant Leadership",
                "Charismatic Leadership",
                "Empowering Leadership",
                "Participative Leadership",
                "Abusive Supervision",
                "Toxic Leadership"),
  Valence = c("Positive", "Positive", "Positive", "Positive", "Positive", 
              "Positive", "Positive", "Negative", "Negative"),
  "Number of Items" = c(45, 38, 32, 42, 28, 26, 24, 35, 42),
  "Example Dimensions" = c("Idealized influence, Inspirational motivation",
                           "Self-awareness, Relational transparency",
                           "Ethical conduct, Moral management",
                           "Stewardship, Empowerment",
                           "Vision articulation, Environmental sensitivity",
                           "Delegation, Coaching",
                           "Consultation, Joint decision-making",
                           "Hostile verbal behavior, Public criticism",
                           "Self-promotion, Unpredictability"),
  stringsAsFactors = FALSE
)

names(leadership_data)[3] <- "Number of Items"
names(leadership_data)[4] <- "Example Dimensions"

create_table(leadership_data, "Leadership constructs included in the semantic analysis with valence, item counts, and example dimensions measured")
```

```{r tbl-holdout-approaches}
#| label: tbl-holdout-approaches
#| tbl-cap: "Comparison of Holdout Validation Approaches"

holdout_data <- data.frame(
  Approach = c("Stratified Item Holdout", "Complete Construct Holdout"),
  "Training Data" = c("90% of items (2,962) from all 246 constructs",
                      "All items from 236 constructs"),
  "Holdout Data" = c("10% of items (380) from all 246 constructs",
                     "All items from 10 held-out constructs"),
  "Primary Advantage" = c("Controls for sample size effects",
                         "Tests novel construct generalization"),
  "Validation Focus" = c("Item-level generalization within known constructs",
                        "Construct-level generalization to novel dimensions"),
  stringsAsFactors = FALSE
)

names(holdout_data)[2] <- "Training Data"
names(holdout_data)[3] <- "Holdout Data"
names(holdout_data)[4] <- "Primary Advantage"
names(holdout_data)[5] <- "Validation Focus"

create_table(holdout_data, "Dual holdout validation framework specifications and rationale")
```

```{r tbl-performance-comparison}
#| label: tbl-performance-comparison
#| tbl-cap: "Construct Separation Performance Comparison"

performance_data <- data.frame(
  Domain = c("IPIP Personality", "Leadership Constructs", "Performance Difference"),
  "Clustering Accuracy" = c("94.70%", "66.33%", "28.37 pp"),
  "95% CI" = c("[92.3, 96.8]", "[62.1, 70.5]", "[24.2, 32.5]"),
  "Adjusted Rand Index" = c("0.78", "0.31", "0.47"),
  "Cohen's d" = c("1.562", "0.409", "2.14"),
  "Effect Size" = c("Large", "Small-Medium", "Very Large"),
  stringsAsFactors = FALSE
)

create_table(performance_data, "Performance metrics comparing construct separation between personality and leadership domains")
```
